# Week 8 - Training neural networks

## Plan for the Week
- Part 1: Learning theory
  - Is learning feasible?
  - Model capacity
  - Bias - variance

- Part 2: Practical aspects of learning
  - Overfitting
  - Evaluating performance
  - Learning from small datasets

- Part 3: Miscellaneous
  - Rethinking generalization
  - Capacity of dense neural networks

## Reading material
- Lecture Notes https://www.uio.no/studier/emner/matnat/ifi/IN5400/v20/material/week8/in5400_2020_week8_generalization.pdf

- Read: CS231n:
  - sections “L2 / L1 / Dropouts” - http://cs231n.github.io/neural-networks-2/
  - Transfer learning - http://cs231n.github.io/transfer-learning/

- Read: Multitask learning - https://ruder.io/multi-task/
  - (From “Introduction” to “MTL in non-neural models”)
  - (From “Auxiliary taks” and out)

- Optional
  - Learning theory (caltech course) - https://work.caltech.edu/lectures.html:
    - Lecture (Videos): 2,5,6,7,8,11

  - Read: The Curse of Dimensionality in classification - https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/
  - Read: Rethinking generalization - https://arxiv.org/pdf/1611.03530.pdf

## Resources
