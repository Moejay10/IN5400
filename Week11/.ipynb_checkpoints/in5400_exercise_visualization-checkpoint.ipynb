{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Visualization exercises\n",
    "\n",
    "In this notebook we will explore the use of image gradients for generating new images.\n",
    "\n",
    "When training a model, we define a loss function which measures our current unhappiness with the model's performance; we then use backpropagation to compute the gradient of the loss with respect to the model parameters, and perform gradient descent on the model parameters to minimize the loss.\n",
    "\n",
    "Here we will do something slightly different. We will start from a convolutional neural network model which has been pretrained to perform image classification on the ImageNet dataset. We will use this model to define a loss function which quantifies our current unhappiness with our image, then use backpropagation to compute the gradient of this loss with respect to the pixels of the image. We will then keep the model fixed, and perform gradient descent on the image to synthesize a new image which minimizes the loss.\n",
    "\n",
    "The programs can run on CPU as they are quite light in terms of comptutational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consider using \"conda install numpy=1.16.2\" if you get a problem with unpicke in loading imagenet images.\n",
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.image_utils import SQUEEZENET_MEAN, SQUEEZENET_STD\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and utility functions.\n",
    "\n",
    "We will use a pretrained model, and we need to apply the same type of preprocessing of the images as the model was trained on. This involves subtracting the per-color mean and dividing by the per-color standard deviation. We define a few helper functions for performing and undoing this preprocessing. You don't need to do anything in this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, size=224):\n",
    "    transform = T.Compose([\n",
    "        T.Resize(size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=SQUEEZENET_MEAN.tolist(),\n",
    "                    std=SQUEEZENET_STD.tolist()),\n",
    "        T.Lambda(lambda x: x[None]),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def deprocess(img, should_rescale=True):\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda x: x[0]),\n",
    "        T.Normalize(mean=[0, 0, 0], std=(1.0 / SQUEEZENET_STD).tolist()),\n",
    "        T.Normalize(mean=(-SQUEEZENET_MEAN).tolist(), std=[1, 1, 1]),\n",
    "        T.Lambda(rescale) if should_rescale else T.Lambda(lambda x: x),\n",
    "        T.ToPILImage(),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def rescale(x):\n",
    "    low, high = x.min(), x.max()\n",
    "    x_rescaled = (x - low) / (high - low)\n",
    "    return x_rescaled\n",
    "    \n",
    "def blur_image(X, sigma=1):\n",
    "    X_np = X.cpu().clone().numpy()\n",
    "    X_np = gaussian_filter1d(X_np, sigma, axis=2)\n",
    "    X_np = gaussian_filter1d(X_np, sigma, axis=3)\n",
    "    X.copy_(torch.Tensor(X_np).type_as(X))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Using a pretrained Model\n",
    "\n",
    "For all of our image generation experiments, we will start with a convolutional neural network which was pretrained to perform image classification on ImageNet. We can use any model here, but for the purposes of this assignment we will use SqueezeNet [1], which achieves accuracies comparable to AlexNet but with a significantly reduced parameter count and computational complexity.\n",
    "\n",
    "Using SqueezeNet rather than AlexNet or VGG or ResNet means that we can easily perform all image generation experiments on CPU.\n",
    "\n",
    "[1] Iandola et al, \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 0.5MB model size\", arXiv 2016\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the pretrained SqueezeNet model.\n",
    "model = torchvision.models.squeezenet1_1(pretrained=True)\n",
    "\n",
    "# Tell PyTorch not to compute gradients\n",
    "# with respect to model parameters.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# you may see warning regarding initialization deprecated, that's fine, please continue to next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load some ImageNet images\n",
    "\n",
    "We have provided a few example images from the validation set of the ImageNet ILSVRC 2012 Classification dataset. \n",
    "These images are found at /projects/in5400/visualization, and they are also included in the zip-fil of this solution.\n",
    "\n",
    "\n",
    "\n",
    "Since they come from the validation set, our pretrained model did not see these images during training.\n",
    "\n",
    "Run the following cell to visualize some of these images, along with their ground-truth labels.\n",
    "\n",
    "Update 2020: The images are supplied in the zip-file for the exercises. No need to download them from /projects/in5400/visualization. \n",
    "The imagenet examples are read from /projects/in5400/visualization/imagenet_val_25.npz. Change the directory in function utils/data_utils.py if you are not using the linux computers in Module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import load_imagenet_val\n",
    "X, y, class_names = load_imagenet_val(num=25)\n",
    "\n",
    "X_subset = X[13:18]\n",
    "y_subset = y[13:18]\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(X_subset[i])\n",
    "    plt.title(class_names[y_subset[i]])\n",
    "    plt.axis('off')\n",
    "plt.gcf().tight_layout()\n",
    "\n",
    "X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Adverserial fooling\n",
    "\n",
    "Your first task is to implement simple adverserial fooling. \n",
    "We will use image gradients to generate \"fooling images\" as discussed in [3]. Given an image and a target class, we can perform gradient ascent over the image to maximize the target class, stopping when the network classifies the image as the target class. Implement the following function to generate fooling images.\n",
    "\n",
    "[3] Szegedy et al, \"Intriguing properties of neural networks\", ICLR 2014\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fastgradsign_image(X, target_y, model):\n",
    "    \"\"\"\n",
    "    Generate an adverserial image that is close to X, but that the model classifies\n",
    "    as target_y.\n",
    "\n",
    "    Inputs:\n",
    "    - X: Input image; Tensor of shape (1, 3, 224, 224)\n",
    "    - target_y: An integer in the range [0, 1000)\n",
    "    - model: A pretrained CNN\n",
    "\n",
    "    Returns:\n",
    "    - X_adver: An image that is close to X, but that is classifed as target_y\n",
    "    by the model.\n",
    "    \"\"\"\n",
    "    # Initialize our fooling image to the input image, and make it require gradient\n",
    "    X_adver = X.clone()\n",
    "    X_adver = X_adver.requires_grad_()\n",
    "    \n",
    "    epsilon = 0\n",
    "    ##############################################################################\n",
    "    # TODO: Generate an adverserial image X_adver that the model will classify as     #\n",
    "    # the class target_y. You should perform gradient ascent on the score of the #\n",
    "    # target class, stopping when the model is fooled.                           #\n",
    "    # epsilon tells you how much you need to modify X, loop over iterations      #\n",
    "    #         and stop when the classifier is fooled (predicting target_y.       #\n",
    "    # For each iteration, take a step of size epsilon in the direction of the    #        \n",
    "    # negative gradient grad                                                     #\n",
    "    # You should write a training loop.                                          #\n",
    "    #                                                                            #\n",
    "    # HINT: For most examples, you should be able to generate a fooling image    #\n",
    "    # in fewer than 100 iterations of gradient ascent.                           #\n",
    "    # You can print your progress over iterations to check your algorithm.       #\n",
    "    ##############################################################################\n",
    "    \n",
    "    \n",
    "    ## Do a number of iterations\n",
    "   \n",
    "        \n",
    "        # Forward pass X_Adver to get the scores\n",
    "    \n",
    "        #Get the index of the best class\n",
    "        \n",
    "        # If the maximum score (index[0]) is equal to the target, break\n",
    "        \n",
    "    \n",
    "        # Get the score corresponding to the target class\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        #Do a backward pass of target_score\n",
    "        \n",
    "    \n",
    "        # Generate the gradient of X_adver\n",
    "       \n",
    "    \n",
    "        # Update the image using gradient ascent\n",
    "        \n",
    "       \n",
    "        \n",
    "        # zero out the gradients of X_adver for the next iteration\n",
    "        \n",
    "\n",
    "\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    return X_adver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to generate an adverserial image. You should ideally see at first glance no major difference between the original and adverserial images, and the network should now make an incorrect prediction on the adverserial one. However you should see a bit of random noise if you look at the 10x magnified difference between the original and fooling images. Feel free to change the idx variable to explore other images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "#idx = 13\n",
    "target_y = 6\n",
    "#target_y = 56\n",
    "#target_y = 472\n",
    "\n",
    "X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\n",
    "X_adver = make_fastgradsign_image(X_tensor[idx:idx+1], target_y, model)\n",
    "\n",
    "scores = model(X_adver)\n",
    "assert target_y == scores.data.max(1)[1][0].item(), 'The model is not fooled!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us visualize the original and the adverserial image, with magnifications that highlights the modifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_adver_np = deprocess(X_adver.clone())\n",
    "X_adver_np = np.asarray(X_adver_np).astype(np.uint8)\n",
    "\n",
    "plt.subplot(1, 5, 1)\n",
    "plt.imshow(X[idx])\n",
    "plt.title(class_names[y[idx]])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 5, 2)\n",
    "plt.imshow(X_adver_np)\n",
    "plt.title(class_names[target_y])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 5, 3)\n",
    "X_pre = preprocess(Image.fromarray(X[idx]))\n",
    "diff = np.asarray(deprocess(X_adver - X_pre, should_rescale=False))\n",
    "plt.imshow(diff)\n",
    "plt.title('Difference')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 5, 4)\n",
    "diff = np.asarray(deprocess(10 * (X_adver - X_pre), should_rescale=False))\n",
    "plt.imshow(diff)\n",
    "plt.title('Magnified difference (10x)')\n",
    "plt.axis('off')\n",
    "\n",
    "#plt.subplot(1, 5, 5)\n",
    "#plt.imshow(X[target_y])\n",
    "#plt.title('target original')\n",
    "#plt.axis('off')\n",
    "\n",
    "plt.gcf().set_size_inches(12, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create saliency maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this pretrained model, we will compute class saliency maps as described in Section 3.1 of [2].\n",
    "\n",
    "A saliency map tells us the degree to which each pixel in the image affects the classification score for that image. To compute it, we compute the gradient of the unnormalized score corresponding to the correct class (which is a scalar) with respect to the pixels of the image. If the image has shape (3, H, W) then this gradient will also have shape (3, H, W); for each pixel in the image, this gradient tells us the amount by which the classification score will change if the pixel changes by a small amount. To compute the saliency map, we take the absolute value of this gradient, then take the maximum value over the 3 input channels; the final saliency map thus has shape (H, W) and all entries are nonnegative.\n",
    "\n",
    "[2] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. \"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\", ICLR Workshop 2014.\n",
    "\n",
    "\n",
    "Hint: PyTorch gather method\n",
    "\n",
    "Here we need to select one element from each row of a matrix; if s is an numpy array of shape (N, C) and y is a numpy array of shape (N,) containing integers 0 <= y[i] < C, then s[np.arange(N), y] is a numpy array of shape (N,) which selects one element from each element in s using the indices in y.\n",
    "\n",
    "In PyTorch you can perform the same operation using the gather() method. If s is a PyTorch Tensor of shape (N, C) and y is a PyTorch Tensor of shape (N,) containing longs in the range 0 <= y[i] < C, then\n",
    "\n",
    "s.gather(1, y.view(-1, 1)).squeeze()\n",
    "\n",
    "will be a PyTorch Tensor of shape (N,) containing one entry from each row of s, selected according to the indices in y.\n",
    "\n",
    "\n",
    "\n",
    "You can also read the documentation for the gather method and the squeeze method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency(X, y, model):\n",
    "    \"\"\"\n",
    "    Compute a class saliency map using the model for images X and labels y.\n",
    "\n",
    "    Input:\n",
    "    - X: Input images; Tensor of shape (N, 3, H, W)\n",
    "    - y: Labels for X; LongTensor of shape (N,)\n",
    "    - model: A pretrained CNN that will be used to compute the saliency map.\n",
    "\n",
    "    Returns:\n",
    "    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n",
    "    images.\n",
    "    \"\"\"\n",
    "    # Make sure the model is in \"test\" mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Make input tensor require gradient\n",
    "    X.requires_grad_()\n",
    "    \n",
    "    saliency = None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement this function. Perform a forward and backward pass through #\n",
    "    # the model to compute the gradient of the correct class score with respect  #\n",
    "    # to each input image. You first want to compute the loss over the correct   #\n",
    "    # scores (we'll combine losses across a batch by summing), and then compute  #\n",
    "    # the gradients with a backward pass.                                        #\n",
    "    ##############################################################################\n",
    "    \n",
    "    # Forward propagete X to get scores\n",
    "    \n",
    "    \n",
    "    # Use gather to get the scores for a class y\n",
    "   \n",
    "    # Create a tensor from this\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # Backpropagate this tensor\n",
    "    \n",
    "    # Find saliency as the gradient with of X\n",
    "    \n",
    "    # Take the absolute value over the input channels of X \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    return saliency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed the implementation in the cell above, run the following to visualize some class saliency maps on our example images from the ImageNet validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_saliency_maps(X, y):\n",
    "    # Convert X and y from numpy arrays to Torch Tensors\n",
    "    X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "\n",
    "    # Compute saliency maps for images in X\n",
    "    saliency = compute_saliency(X_tensor, y_tensor, model)\n",
    "\n",
    "    # Convert the saliency map from Torch Tensor to numpy array and show images\n",
    "    # and saliency maps together.\n",
    "    saliency = saliency.numpy()\n",
    "    N = X.shape[0]\n",
    "    N=5\n",
    "    for i in range(N):\n",
    "        plt.subplot(2, N, i + 1)\n",
    "        plt.imshow(X[i])\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y[i]])\n",
    "        plt.subplot(2, N, N + i + 1)\n",
    "        plt.imshow(saliency[i], cmap=plt.cm.hot)\n",
    "        plt.axis('off')\n",
    "        plt.gcf().set_size_inches(12, 5)\n",
    "    plt.show()\n",
    "\n",
    "display_saliency_maps(X_subset,y_subset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using gradient ascent on the image to visualize a target class\n",
    "\n",
    "By starting with a random noise image and performing gradient ascent on a target class, we can generate an image that the network will recognize as the target class. This idea was first presented in [2]; [3] extended this idea by suggesting several regularization techniques that can improve the quality of the generated image.\n",
    "\n",
    "Concretely, let $I$ be an image and let $y$ be a target class. Let $s_y(I)$ be the score that a convolutional network assigns to the image $I$ for class $y$; note that these are raw unnormalized scores, not class probabilities. We wish to generate an image $I^*$ that achieves a high score for the class $y$ by solving the problem\n",
    "\n",
    "$$\n",
    "I^* = \\arg\\max_I (s_y(I) - R(I))\n",
    "$$\n",
    "\n",
    "where $R$ is a (possibly implicit) regularizer (note the sign of $R(I)$ in the argmax: we want to minimize this regularization term). We can solve this optimization problem using gradient ascent, computing gradients with respect to the generated image. We will use (explicit) L2 regularization of the form\n",
    "\n",
    "$$\n",
    "R(I) = \\lambda \\|I\\|_2^2\n",
    "$$\n",
    "\n",
    "**and** implicit regularization as suggested by [3] by periodically blurring the generated image. We can solve this problem using gradient ascent on the generated image.\n",
    "\n",
    "In the cell below, complete the implementation of the generate_class_visualization` function.\n",
    "\n",
    "[2] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. \"Deep Inside Convolutional Networks: Visualising\n",
    "Image Classification Models and Saliency Maps\", ICLR Workshop 2014.\n",
    "\n",
    "[3] Yosinski et al, \"Understanding Neural Networks Through Deep Visualization\", ICML 2015 Deep Learning Workshop\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitter(X, ox, oy):\n",
    "    \"\"\"\n",
    "    Helper function to randomly jitter an image.\n",
    "    \n",
    "    Inputs\n",
    "    - X: PyTorch Tensor of shape (N, C, H, W)\n",
    "    - ox, oy: Integers giving number of pixels to jitter along W and H axes\n",
    "    \n",
    "    Returns: A new PyTorch Tensor of shape (N, C, H, W)\n",
    "    \"\"\"\n",
    "    if ox != 0:\n",
    "        left = X[:, :, :, :-ox]\n",
    "        right = X[:, :, :, -ox:]\n",
    "        X = torch.cat([right, left], dim=3)\n",
    "    if oy != 0:\n",
    "        top = X[:, :, :-oy]\n",
    "        bottom = X[:, :, -oy:]\n",
    "        X = torch.cat([bottom, top], dim=2)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_visualization(target_y, model, dtype, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate an image to maximize the score of target_y under a pretrained model.\n",
    "    \n",
    "    Inputs:\n",
    "    - target_y: Integer in the range [0, 1000) giving the index of the class\n",
    "    - model: A pretrained CNN that will be used to generate the image\n",
    "    - dtype: Torch datatype to use for computations\n",
    "    \n",
    "    Keyword arguments:\n",
    "    - l2_reg: Strength of L2 regularization on the image\n",
    "    - learning_rate: How big of a step to take\n",
    "    - num_iterations: How many iterations to use\n",
    "    - blur_every: How often to blur the image as an implicit regularizer\n",
    "    - max_jitter: How much to gjitter the image as an implicit regularizer\n",
    "    - show_every: How often to show the intermediate result\n",
    "    \"\"\"\n",
    "    model.type(dtype)\n",
    "    \n",
    "    l2_reg = kwargs.pop('l2_reg',1e-3)\n",
    "    learning_rate = kwargs.pop('learning_rate', 50)\n",
    "    num_iterations = kwargs.pop('num_iterations', 400)\n",
    "    blur_every = kwargs.pop('blur_every', 20)\n",
    "    max_jitter = kwargs.pop('max_jitter', 20)\n",
    "    show_every = kwargs.pop('show_every', 25)\n",
    "\n",
    "    # Randomly initialize the image as a PyTorch Tensor, and make it requires gradient.\n",
    "    img = torch.randn(1, 3, 224, 224).mul_(1.0).type(dtype).requires_grad_()\n",
    "\n",
    "    for t in range(num_iterations):\n",
    "        # Randomly jitter the image a bit; this gives slightly nicer results\n",
    "        ox, oy = random.randint(0, max_jitter), random.randint(0, max_jitter)\n",
    "        img.data.copy_(jitter(img.data, ox, oy))\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO: Use the model to compute the gradient of the score for the     #\n",
    "        # class target_y with respect to the pixels of the image, and make a   #\n",
    "        # gradient step on the image using the learning rate. Don't forget the #\n",
    "        # L2 regularization term!                                              #\n",
    "        # Be very careful about the signs of elements in your code.            #\n",
    "        ########################################################################\n",
    "        \n",
    "        #forward pass\n",
    "        \n",
    "        # Get the score for the target class and backpropagate only this\n",
    "       \n",
    "        \n",
    "        # Do a backward pass to get the image gradients\n",
    "       \n",
    "        \n",
    "        # Get gradients of the image\n",
    "        \n",
    "        \n",
    "        # Add l2 regularization derivative term\n",
    "        \n",
    "        \n",
    "        # NOrmalize tha gradients in gradient ascent update\n",
    "        \n",
    "        \n",
    "        # zero out the gradients\n",
    "       \n",
    "        \n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################\n",
    "        \n",
    "        # Undo the random jitter\n",
    "        img.data.copy_(jitter(img.data, -ox, -oy))\n",
    "\n",
    "        # As regularizer, clamp and periodically blur the image\n",
    "        for c in range(3):\n",
    "            lo = float(-SQUEEZENET_MEAN[c] / SQUEEZENET_STD[c])\n",
    "            hi = float((1.0 - SQUEEZENET_MEAN[c]) / SQUEEZENET_STD[c])\n",
    "            img.data[:, c].clamp_(min=lo, max=hi)\n",
    "        if t % blur_every == 0:\n",
    "            blur_image(img.data, sigma=0.5)\n",
    "        \n",
    "        # Periodically show the image\n",
    "        if t == 0 or (t + 1) % show_every == 0 or t == num_iterations - 1:\n",
    "            plt.imshow(deprocess(img.data.clone().cpu()))\n",
    "            class_name = class_names[target_y]\n",
    "            plt.title('%s\\nIteration %d / %d' % (class_name, t + 1, num_iterations))\n",
    "            plt.gcf().set_size_inches(4, 4)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    return deprocess(img.data.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed the implementation in the cell above, run the following cell to generate an image of a Gorilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor # Uncomment this to use GPU\n",
    "model.type(dtype)\n",
    "\n",
    "#target_y = 76 # Tarantula\n",
    "#target_y = 78 # Tick\n",
    "#\n",
    "#target_y = 187 # Yorkshire Terrier\n",
    "# target_y = 683 # Oboe\n",
    "target_y = 366 # Gorilla\n",
    "#target_y = 604 # Hourglass\n",
    "#target_y = 251\n",
    "out = generate_class_visualization(target_y, model, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
